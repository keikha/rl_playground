{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import statistics\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "StateAction = namedtuple(\"StateAction\", [\"state\", \"action\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descretize2(observation, no_states=[1, 8, 8, 8]):\n",
    "    obs_min = [-2, -2.5, -0.41 , -2]\n",
    "    obs_max = [2, 2.5, 0.41 , 2]\n",
    "    \n",
    "    states =  [math.floor(no_states[i] * ((observation[i] - obs_min[i])/(obs_max[i]-obs_min[i])))     for i in range(len(observation)) ]\n",
    "    print( [ ((observation[i] - obs_min[i])/(obs_max[i]-obs_min[i]))     for i in range(len(observation)) ])\n",
    "    print(states)\n",
    "    s = sum(math.pow(10, i-1) * states[i] for i in range(len(states)))\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descretize(observation, no_states=[1, 8, 8, 8]):\n",
    "    obs_min = [-2, -2.5, -0.41 , -2]\n",
    "    obs_max = [2, 2.5, 0.41 , 2]\n",
    "    \n",
    "    states =  [math.floor(no_states[i] * ((observation[i] - obs_min[i])/(obs_max[i]-obs_min[i])))  for i in range(len(observation)) ]\n",
    "\n",
    "    c = [i for i in range(len(states)) if states[i] >= no_states[i]]\n",
    "    d = [i for i in range(len(states)) if states[i] <0 ]\n",
    "\n",
    "    for i in c: \n",
    "        states[i] = no_states[i]-1\n",
    "\n",
    "    for i in d:\n",
    "        states[i] = 0\n",
    "    \n",
    "    return  np.ravel_multi_index(states, no_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy_render(policy, n_episodes = 10):\n",
    "    ep_len = []\n",
    "    for i_episode in range(n_episodes):\n",
    "        observation = env.reset()\n",
    "        s = descretize(observation)\n",
    "        t = 0\n",
    "        while True:\n",
    "            t += 1\n",
    "            env.render()\n",
    "            if s in policy:\n",
    "                action = policy[s]\n",
    "            else:\n",
    "                print('state not found')\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            s = descretize(observation)\n",
    "            if done:\n",
    "                ep_len.append(t)\n",
    "                break\n",
    "    env.close()\n",
    "    print(\"mean of episodes: {}: \".format(statistics.mean(ep_len)))\n",
    "    print(\"median of episodes: {}: \".format(statistics.median(ep_len)))\n",
    "    print(\"svd of episodes: {}: \".format(statistics.stdev(ep_len)))\n",
    "    print(\"max of episodes: {}: \".format(max(ep_len)))\n",
    "    print(\"min of episodes: {}: \".format(min(ep_len)))\n",
    "    print(\"no of states with value: \", len(policy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy_no_render(policy, n_episodes = 100):\n",
    "    ep_len = []\n",
    "    for i_episode in range(n_episodes):\n",
    "        observation = env.reset()\n",
    "        s = descretize(observation)\n",
    "        t = 0\n",
    "        while True:\n",
    "            t += 1\n",
    "            if s in policy:\n",
    "                action = policy[s]\n",
    "            else:\n",
    "                print('state not found')\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            s = descretize(observation)\n",
    "            if done:\n",
    "                ep_len.append(t)\n",
    "                break\n",
    "    env.close()\n",
    "    print(\"mean of episodes: {}: \".format(statistics.mean(ep_len)))\n",
    "    print(\"median of episodes: {}: \".format(statistics.median(ep_len)))\n",
    "    print(\"svd of episodes: {}: \".format(statistics.stdev(ep_len)))\n",
    "    print(\"max of episodes: {}: \".format(max(ep_len)))\n",
    "    print(\"min of episodes: {}: \".format(min(ep_len)))\n",
    "    print(\"no of states with value: \", len(policy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc(n_episodes = 5000):\n",
    "    state_action_returns = defaultdict(list)\n",
    "    state_action_value =  defaultdict(float)\n",
    "    policy = {}\n",
    "    gamma = 0.99\n",
    "\n",
    "    for i_episode in range(n_episodes):\n",
    "        if(i_episode%100 == 0):\n",
    "            print('running episode:', i_episode )\n",
    "        observation = env.reset()\n",
    "        t = 0\n",
    "        path = []\n",
    "        while True:\n",
    "            t += 1\n",
    "            action = env.action_space.sample()\n",
    "            observation_next, reward, done, info = env.step(action)\n",
    "            path.append((descretize(observation), action, reward))\n",
    "            observation = observation_next\n",
    "            \n",
    "            if done:\n",
    "                g = 0\n",
    "                for i in range(len(path), 0, -1):\n",
    "                    state, action, reward = path[i-1]\n",
    "                    g = reward + gamma * g \n",
    "                    sa = StateAction(state = state, action = action)\n",
    "                    state_action_returns[sa].append(g)\n",
    "                    new_mean = statistics.mean(state_action_returns[sa]) \n",
    "                    if new_mean > state_action_value[sa]:\n",
    "                        state_action_value[sa] = new_mean\n",
    "                        policy[state] = action\n",
    "\n",
    "                break\n",
    "    env.close()\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running episode: 0\n",
      "running episode: 100\n",
      "running episode: 200\n",
      "running episode: 300\n",
      "running episode: 400\n",
      "running episode: 500\n",
      "running episode: 600\n",
      "running episode: 700\n",
      "running episode: 800\n",
      "running episode: 900\n",
      "running episode: 1000\n",
      "running episode: 1100\n",
      "running episode: 1200\n",
      "running episode: 1300\n",
      "running episode: 1400\n",
      "running episode: 1500\n",
      "running episode: 1600\n",
      "running episode: 1700\n",
      "running episode: 1800\n",
      "running episode: 1900\n",
      "running episode: 2000\n",
      "running episode: 2100\n",
      "running episode: 2200\n",
      "running episode: 2300\n",
      "running episode: 2400\n",
      "running episode: 2500\n",
      "running episode: 2600\n",
      "running episode: 2700\n",
      "running episode: 2800\n",
      "running episode: 2900\n",
      "running episode: 3000\n",
      "running episode: 3100\n",
      "running episode: 3200\n",
      "running episode: 3300\n",
      "running episode: 3400\n",
      "running episode: 3500\n",
      "running episode: 3600\n",
      "running episode: 3700\n",
      "running episode: 3800\n",
      "running episode: 3900\n",
      "running episode: 4000\n",
      "running episode: 4100\n",
      "running episode: 4200\n",
      "running episode: 4300\n",
      "running episode: 4400\n",
      "running episode: 4500\n",
      "running episode: 4600\n",
      "running episode: 4700\n",
      "running episode: 4800\n",
      "running episode: 4900\n",
      "no of covered states:  180\n"
     ]
    }
   ],
   "source": [
    "policy = mc()\n",
    "print(\"no of covered states: \", len(policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of episodes: 32.91: \n",
      "median of episodes: 15.0: \n",
      "svd of episodes: 36.33794660731896: \n",
      "max of episodes: 156: \n",
      "min of episodes: 8: \n",
      "no of states with value:  180\n"
     ]
    }
   ],
   "source": [
    "test_policy_no_render(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_0(n_episodes = 5000):\n",
    "    \n",
    "    state_action_value = defaultdict(list)\n",
    "    \n",
    "    for i_episode in range(n_episodes):\n",
    "        if(i_episode%500 == 0):\n",
    "            print('running episode:', i_episode )\n",
    "        observation = env.reset()\n",
    "        state = descretize(observation)\n",
    "        action = take_action_e_greedy(state_action_value, state)\n",
    "        done = False\n",
    "        while not done:\n",
    "            observation_next, reward, done, info = env.step(action)\n",
    "            action_next = take_action_e_greedy(state_action_value, descretize(observation_next))\n",
    "            current_value = state_action_value[s]\n",
    "    \n",
    "    state_action_value =  defaultdict(float)\n",
    "    policy = {}\n",
    "    gamma = 0.99\n",
    "\n",
    "    for i_episode in range(n_episodes):\n",
    "        if(i_episode%100 == 0):\n",
    "            print('running episode:', i_episode )\n",
    "        observation = env.reset()\n",
    "        t = 0\n",
    "        path = []\n",
    "        while True:\n",
    "            t += 1\n",
    "            action = env.action_space.sample()\n",
    "            observation_next, reward, done, info = env.step(action)\n",
    "            path.append((descretize(observation), action, reward))\n",
    "            observation = observation_next\n",
    "            \n",
    "            if done:\n",
    "                g = 0\n",
    "                for i in range(len(path), 0, -1):\n",
    "                    state, action, reward = path[i-1]\n",
    "                    g = reward + gamma * g \n",
    "                    sa = StateAction(state = state, action = action)\n",
    "                    state_action_returns[sa].append(g)\n",
    "                    new_mean = statistics.mean(state_action_returns[sa]) \n",
    "                    if new_mean > state_action_value[sa]:\n",
    "                        state_action_value[sa] = new_mean\n",
    "                        policy[state] = action\n",
    "\n",
    "                break\n",
    "    env.close()\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action_e_greedy(state_action_value, state, epsilon = 0.1):\n",
    "    if state not in state_action_value:\n",
    "        state_action_value[state] = np.random.random_sample(2)/20\n",
    "    values = state_action_value[state]\n",
    "    max_index = values.index(max(values))\n",
    "    min_index = values.index(min(values))\n",
    "    if np.random.random < epsilon:\n",
    "        return min_index\n",
    "    else:\n",
    "        return max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0458275 , 0.00032388])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_len = []\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "value={}\n",
    "initial_value = random.uniform(-0.05, 0.05)\n",
    "\n",
    "for i_episode in range(10000):\n",
    "    observation = env.reset()\n",
    "    prev_s = round(observation[2],2)\n",
    "    t = 0\n",
    "    while True:\n",
    "        t += 1\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        state = round(observation[2],2)\n",
    "        v_p_s = value.get(prev_s, initial_value)\n",
    "        v_n_s = value.get(state, initial_value)\n",
    "        value[prev_s] = v_p_s + alpha*(reward + gamma*v_n_s - v_p_s)\n",
    "        prev_s = state\n",
    "        if done:\n",
    "            ep_len.append(t)\n",
    "            break\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, parameters):  \n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "    while True:\n",
    "        action = 0 if np.matmul(parameters,observation) < 0 else 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0\n",
      "[-0.15494122 -0.15297247  0.76247454  0.67769423]\n"
     ]
    }
   ],
   "source": [
    "bestparams = None  \n",
    "bestreward = 0  \n",
    "for _ in range(10000):  \n",
    "    parameters = np.random.rand(4) * 2 - 1\n",
    "    reward = run_episode(env,parameters)\n",
    "    if reward > bestreward:\n",
    "        bestreward = reward\n",
    "        bestparams = parameters\n",
    "        # considered solved if the agent lasts 200 timesteps\n",
    "#         if reward == 200:\n",
    "#             break\n",
    "print (bestreward)\n",
    "print (bestparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of episodes: 171.1649: \n",
      "median of episodes: 200.0: \n",
      "svd of episodes: 43.65466849030124: \n",
      "max of episodes: 200.0: \n",
      "min of episodes: 78.0: \n"
     ]
    }
   ],
   "source": [
    "ep_len = []\n",
    "for _ in range(10000):  \n",
    "    reward = run_episode(env, bestparams)\n",
    "    # considered solved if the agent lasts 200 timesteps\n",
    "    ep_len.append(reward)\n",
    "print(\"mean of episodes: {}: \".format(statistics.mean(ep_len)))\n",
    "print(\"median of episodes: {}: \".format(statistics.median(ep_len)))\n",
    "print(\"svd of episodes: {}: \".format(statistics.stdev(ep_len)))\n",
    "print(\"max of episodes: {}: \".format(max(ep_len)))\n",
    "print(\"min of episodes: {}: \".format(min(ep_len)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch():\n",
    "    noise_scaling = 0.1  \n",
    "    parameters = np.random.rand(4) * 2 - 1  \n",
    "    bestreward = 0 \n",
    "    \n",
    "    for _i in range(10000):  \n",
    "        newparams = parameters + (np.random.rand(4) * 2 - 1)*noise_scaling\n",
    "        reward = 0\n",
    "        run = run_episode(env,newparams)\n",
    "        if reward > bestreward:\n",
    "            bestreward = reward\n",
    "            parameters = newparams\n",
    "            if sdsreward == 200:\n",
    "                break\n",
    "    return _i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
